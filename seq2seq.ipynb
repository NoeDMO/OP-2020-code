{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorm0202/OP-2020-code/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ALSjdKLQ5SB"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import sys\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF2YMewKQ5SC"
      },
      "source": [
        "def load_data(source, dist, max_len, vocab_size):\n",
        "\n",
        "    # Reading raw text from source and destination files\n",
        "    f = open(source, 'r')\n",
        "    X_data = f.read()\n",
        "    f.close()\n",
        "    f = open(dist, 'r')\n",
        "    y_data = f.read()\n",
        "    f.close()\n",
        "\n",
        "    # Splitting raw text into array of sequences\n",
        "    X = [text_to_word_sequence(x)[::-1] for x, y in zip(X_data.split('\\n'), y_data.split('\\n')) if len(x) > 0 and len(y) > 0 and len(x) <= max_len and len(y) <= max_len]\n",
        "    y = [text_to_word_sequence(y) for x, y in zip(X_data.split('\\n'), y_data.split('\\n')) if len(x) > 0 and len(y) > 0 and len(x) <= max_len and len(y) <= max_len]\n",
        "\n",
        "    # Creating the vocabulary set with the most common words\n",
        "    dist = FreqDist(np.hstack(X))\n",
        "    X_vocab = dist.most_common(vocab_size-1)\n",
        "    dist = FreqDist(np.hstack(y))\n",
        "    y_vocab = dist.most_common(vocab_size-1)\n",
        "\n",
        "    # Creating an array of words from the vocabulary set, we will use this array as index-to-word dictionary\n",
        "    X_ix_to_word = [word[0] for word in X_vocab]\n",
        "    # Adding the word \"ZERO\" to the beginning of the array\n",
        "    X_ix_to_word.insert(0, 'ZERO')\n",
        "    # Adding the word 'UNK' to the end of the array (stands for UNKNOWN words)\n",
        "    X_ix_to_word.append('UNK')\n",
        "\n",
        "    # Creating the word-to-index dictionary from the array created above\n",
        "    X_word_to_ix = {word:ix for ix, word in enumerate(X_ix_to_word)}\n",
        "\n",
        "    # Converting each word to its index value\n",
        "    for i, sentence in enumerate(X):\n",
        "        for j, word in enumerate(sentence):\n",
        "            if word in X_word_to_ix:\n",
        "                X[i][j] = X_word_to_ix[word]\n",
        "            else:\n",
        "                X[i][j] = X_word_to_ix['UNK']\n",
        "\n",
        "    y_ix_to_word = [word[0] for word in y_vocab]\n",
        "    y_ix_to_word.insert(0, 'ZERO')\n",
        "    y_ix_to_word.append('UNK')\n",
        "    y_word_to_ix = {word:ix for ix, word in enumerate(y_ix_to_word)}\n",
        "    for i, sentence in enumerate(y):\n",
        "        for j, word in enumerate(sentence):\n",
        "            if word in y_word_to_ix:\n",
        "                y[i][j] = y_word_to_ix[word]\n",
        "            else:\n",
        "                y[i][j] = y_word_to_ix['UNK']\n",
        "    return (X, len(X_vocab)+2, X_word_to_ix, X_ix_to_word, y, len(y_vocab)+2, y_word_to_ix, y_ix_to_word)\n",
        "\n",
        "def load_test_data(source, X_word_to_ix, max_len):\n",
        "    f = open(source, 'r')\n",
        "    X_data = f.read()\n",
        "    f.close()\n",
        "\n",
        "    X = [text_to_word_sequence(x)[::-1] for x in X_data.split('\\n') if len(x) > 0 and len(x) <= max_len]\n",
        "    for i, sentence in enumerate(X):\n",
        "        for j, word in enumerate(sentence):\n",
        "            if word in X_word_to_ix:\n",
        "                X[i][j] = X_word_to_ix[word]\n",
        "            else:\n",
        "                X[i][j] = X_word_to_ix['UNK']\n",
        "    return X\n",
        "\n",
        "def find_checkpoint_file(folder):\n",
        "    checkpoint_file = [f for f in os.listdir(folder) if 'checkpoint' in f]\n",
        "    if len(checkpoint_file) == 0:\n",
        "        return []\n",
        "    modified_time = [os.path.getmtime(f) for f in checkpoint_file]\n",
        "    return checkpoint_file[np.argmax(modified_time)]\n",
        "\n",
        "def process_data(word_sentences, max_len, word_to_ix):\n",
        "    # Vectorizing each element in each sequence\n",
        "    sequences = np.zeros((len(word_sentences), max_len, len(word_to_ix)))\n",
        "    for i, sentence in enumerate(word_sentences):\n",
        "        for j, word in enumerate(sentence):\n",
        "            sequences[i, j, word] = 1.\n",
        "    return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_G1ErHOQ5SC"
      },
      "source": [
        "MAX_LEN = 200\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 100\n",
        "LAYER_NUM = 3\n",
        "HIDDEN_DIM = 1000\n",
        "NB_EPOCH = 20\n",
        "MODE = 'train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuAQ3joAQ5SC"
      },
      "source": [
        "# Loading input sequences, output sequences and the necessary mapping dictionaries\n",
        "X, X_vocab_len, X_word_to_ix, X_ix_to_word, y, y_vocab_len, y_word_to_ix, y_ix_to_word = load_data('../data/europarl-v8.fi-en.en', '../data/europarl-v8.fi-en.fi', MAX_LEN, VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYyiGif1Q5SC"
      },
      "source": [
        "# Finding the length of the longest sequence\n",
        "X_max_len = max([len(sentence) for sentence in X])\n",
        "y_max_len = max([len(sentence) for sentence in y])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmdx04JSQ5SC"
      },
      "source": [
        "# Padding zeros to make all sequences have a same length with the longest one\n",
        "X = pad_sequences(X, maxlen=X_max_len, dtype='int32')\n",
        "y = pad_sequences(y, maxlen=y_max_len, dtype='int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbDbua1hQ5SC"
      },
      "source": [
        "# Creating the network model\n",
        "#model = create_model(X_vocab_len, X_max_len, y_vocab_len, y_max_len, HIDDEN_DIM, LAYER_NUM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SDegcNsQ5SC"
      },
      "source": [
        "hidden_size = HIDDEN_DIM\n",
        "num_layers = LAYER_NUM\n",
        "model = Sequential()\n",
        "# Creating encoder network\n",
        "model.add(Embedding(X_vocab_len, 1000, input_length=X_max_len, mask_zero=True))\n",
        "model.add(LSTM(hidden_size))\n",
        "model.add(RepeatVector(y_max_len))\n",
        "\n",
        "# Creating decoder network\n",
        "for _ in range(num_layers):\n",
        "    model.add(LSTM(hidden_size, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(y_vocab_len)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer='rmsprop',\n",
        "        metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY8pTQfpQ5SC",
        "outputId": "760ab6de-6e90-4270-c813-e7814416790a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 47, 1000)          20001000  \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 1000)              8004000   \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 45, 1000)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 45, 1000)          8004000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 45, 1000)          8004000   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 45, 1000)          8004000   \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 45, 20001)         20021001  \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 45, 20001)         0         \n",
            "=================================================================\n",
            "Total params: 72,038,001\n",
            "Trainable params: 72,038,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2QZZqHwQ5SD"
      },
      "source": [
        "# Finding trained weights of previous epoch if any\n",
        "#saved_weights = find_checkpoint_file('.')\n",
        "saved_weights = []\n",
        "#len(saved_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdjgqUfsQ5SD"
      },
      "source": [
        "#checkpoint_file = [f for f in os.listdir('.') if 'checkpoint' in f]\n",
        "#os.listdir('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3babEileRO7z"
      },
      "source": [
        "No se completó el entrenamiento por falta de tiempo... ustedes pueden terminar de entrenarlo. Tarda bastante..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYRAlryQ5SD",
        "outputId": "08d245d8-87dc-4355-84fa-c725dfebffb7"
      },
      "source": [
        "k_start = 1\n",
        "\n",
        "# If any trained weight was found, then load them into the model\n",
        "if len(saved_weights) != 0:\n",
        "    print('[INFO] Saved weights found, loading...')\n",
        "    epoch = saved_weights[saved_weights.rfind('_')+1:saved_weights.rfind('.')]\n",
        "    model.load_weights(saved_weights)\n",
        "    k_start = int(epoch) + 1\n",
        "\n",
        "i_end = 0\n",
        "for k in range(k_start, NB_EPOCH+1):\n",
        "    # Shuffling the training data every epoch to avoid local minima\n",
        "    indices = np.arange(len(X))\n",
        "    np.random.shuffle(indices)\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    # Training 1000 sequences at a time\n",
        "    for i in range(0, len(X), 1000):\n",
        "        if i + 1000 >= len(X):\n",
        "            i_end = len(X)\n",
        "        else:\n",
        "            i_end = i + 1000\n",
        "        y_sequences = process_data(y[i:i_end], y_max_len, y_word_to_ix)\n",
        "\n",
        "        print('[INFO] Training model: epoch {}th {}/{} samples'.format(k, i, len(X)))\n",
        "        model.fit(X[i:i_end], y_sequences, batch_size=BATCH_SIZE, epochs=1, verbose=2)\n",
        "    model.save_weights('checkpoint_epoch_{}.hdf5'.format(k))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Training model: epoch 1th 0/1415032 samples\n",
            "10/10 - 50s - loss: 4.4504 - accuracy: 0.6457\n",
            "[INFO] Training model: epoch 1th 1000/1415032 samples\n",
            "10/10 - 52s - loss: 2.6989 - accuracy: 0.7262\n",
            "[INFO] Training model: epoch 1th 2000/1415032 samples\n",
            "10/10 - 53s - loss: 2.7249 - accuracy: 0.7244\n",
            "[INFO] Training model: epoch 1th 3000/1415032 samples\n",
            "10/10 - 52s - loss: 2.7007 - accuracy: 0.7273\n",
            "[INFO] Training model: epoch 1th 4000/1415032 samples\n",
            "10/10 - 52s - loss: 2.6994 - accuracy: 0.7231\n",
            "[INFO] Training model: epoch 1th 5000/1415032 samples\n",
            "10/10 - 49s - loss: 2.5946 - accuracy: 0.7303\n",
            "[INFO] Training model: epoch 1th 6000/1415032 samples\n",
            "10/10 - 48s - loss: 2.5947 - accuracy: 0.7335\n",
            "[INFO] Training model: epoch 1th 7000/1415032 samples\n",
            "10/10 - 49s - loss: 2.6285 - accuracy: 0.7217\n",
            "[INFO] Training model: epoch 1th 8000/1415032 samples\n",
            "10/10 - 49s - loss: 2.5548 - accuracy: 0.7310\n",
            "[INFO] Training model: epoch 1th 9000/1415032 samples\n",
            "10/10 - 49s - loss: 2.5653 - accuracy: 0.7244\n",
            "[INFO] Training model: epoch 1th 10000/1415032 samples\n",
            "10/10 - 49s - loss: 2.4601 - accuracy: 0.7321\n",
            "[INFO] Training model: epoch 1th 11000/1415032 samples\n",
            "10/10 - 49s - loss: 2.9648 - accuracy: 0.7164\n",
            "[INFO] Training model: epoch 1th 12000/1415032 samples\n",
            "10/10 - 49s - loss: 2.6986 - accuracy: 0.7206\n",
            "[INFO] Training model: epoch 1th 13000/1415032 samples\n",
            "10/10 - 50s - loss: 2.6246 - accuracy: 0.7240\n",
            "[INFO] Training model: epoch 1th 14000/1415032 samples\n",
            "10/10 - 57s - loss: 2.5257 - accuracy: 0.7285\n",
            "[INFO] Training model: epoch 1th 15000/1415032 samples\n",
            "10/10 - 58s - loss: 2.4548 - accuracy: 0.7294\n",
            "[INFO] Training model: epoch 1th 16000/1415032 samples\n",
            "10/10 - 57s - loss: 2.5235 - accuracy: 0.7260\n",
            "[INFO] Training model: epoch 1th 17000/1415032 samples\n",
            "10/10 - 57s - loss: 2.4809 - accuracy: 0.7291\n",
            "[INFO] Training model: epoch 1th 18000/1415032 samples\n",
            "10/10 - 56s - loss: 2.4171 - accuracy: 0.7283\n",
            "[INFO] Training model: epoch 1th 19000/1415032 samples\n",
            "10/10 - 57s - loss: 2.4482 - accuracy: 0.7294\n",
            "[INFO] Training model: epoch 1th 20000/1415032 samples\n",
            "10/10 - 58s - loss: 2.4815 - accuracy: 0.7232\n",
            "[INFO] Training model: epoch 1th 21000/1415032 samples\n",
            "10/10 - 58s - loss: 2.3208 - accuracy: 0.7346\n",
            "[INFO] Training model: epoch 1th 22000/1415032 samples\n",
            "10/10 - 57s - loss: 2.4381 - accuracy: 0.7120\n",
            "[INFO] Training model: epoch 1th 23000/1415032 samples\n",
            "10/10 - 57s - loss: 2.5141 - accuracy: 0.7171\n",
            "[INFO] Training model: epoch 1th 24000/1415032 samples\n",
            "10/10 - 58s - loss: 2.3621 - accuracy: 0.7274\n",
            "[INFO] Training model: epoch 1th 25000/1415032 samples\n",
            "10/10 - 59s - loss: 2.3505 - accuracy: 0.7234\n",
            "[INFO] Training model: epoch 1th 26000/1415032 samples\n",
            "10/10 - 59s - loss: 2.3770 - accuracy: 0.7253\n",
            "[INFO] Training model: epoch 1th 27000/1415032 samples\n",
            "10/10 - 58s - loss: 2.3168 - accuracy: 0.7306\n",
            "[INFO] Training model: epoch 1th 28000/1415032 samples\n",
            "10/10 - 57s - loss: 2.3250 - accuracy: 0.7298\n",
            "[INFO] Training model: epoch 1th 29000/1415032 samples\n",
            "10/10 - 58s - loss: 2.3026 - accuracy: 0.7302\n",
            "[INFO] Training model: epoch 1th 30000/1415032 samples\n",
            "10/10 - 60s - loss: 2.2334 - accuracy: 0.7365\n",
            "[INFO] Training model: epoch 1th 31000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2904 - accuracy: 0.7297\n",
            "[INFO] Training model: epoch 1th 32000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2942 - accuracy: 0.7308\n",
            "[INFO] Training model: epoch 1th 33000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2779 - accuracy: 0.7347\n",
            "[INFO] Training model: epoch 1th 34000/1415032 samples\n",
            "10/10 - 57s - loss: 2.2701 - accuracy: 0.7353\n",
            "[INFO] Training model: epoch 1th 35000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2019 - accuracy: 0.7437\n",
            "[INFO] Training model: epoch 1th 36000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2901 - accuracy: 0.7289\n",
            "[INFO] Training model: epoch 1th 37000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2794 - accuracy: 0.7324\n",
            "[INFO] Training model: epoch 1th 38000/1415032 samples\n",
            "10/10 - 58s - loss: 2.3126 - accuracy: 0.7275\n",
            "[INFO] Training model: epoch 1th 39000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2633 - accuracy: 0.7342\n",
            "[INFO] Training model: epoch 1th 40000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2425 - accuracy: 0.7371\n",
            "[INFO] Training model: epoch 1th 41000/1415032 samples\n",
            "10/10 - 59s - loss: 2.2579 - accuracy: 0.7290\n",
            "[INFO] Training model: epoch 1th 42000/1415032 samples\n",
            "10/10 - 57s - loss: 2.2306 - accuracy: 0.7390\n",
            "[INFO] Training model: epoch 1th 43000/1415032 samples\n",
            "10/10 - 59s - loss: 2.2316 - accuracy: 0.7352\n",
            "[INFO] Training model: epoch 1th 44000/1415032 samples\n",
            "10/10 - 57s - loss: 2.2722 - accuracy: 0.7360\n",
            "[INFO] Training model: epoch 1th 45000/1415032 samples\n",
            "10/10 - 58s - loss: 2.1763 - accuracy: 0.7419\n",
            "[INFO] Training model: epoch 1th 46000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2645 - accuracy: 0.7348\n",
            "[INFO] Training model: epoch 1th 47000/1415032 samples\n",
            "10/10 - 59s - loss: 2.1955 - accuracy: 0.7405\n",
            "[INFO] Training model: epoch 1th 48000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2531 - accuracy: 0.7357\n",
            "[INFO] Training model: epoch 1th 49000/1415032 samples\n",
            "10/10 - 59s - loss: 2.2565 - accuracy: 0.7351\n",
            "[INFO] Training model: epoch 1th 50000/1415032 samples\n",
            "10/10 - 61s - loss: 2.1669 - accuracy: 0.7456\n",
            "[INFO] Training model: epoch 1th 51000/1415032 samples\n",
            "10/10 - 59s - loss: 2.2163 - accuracy: 0.7381\n",
            "[INFO] Training model: epoch 1th 52000/1415032 samples\n",
            "10/10 - 58s - loss: 2.2194 - accuracy: 0.7369\n",
            "[INFO] Training model: epoch 1th 53000/1415032 samples\n",
            "10/10 - 58s - loss: 2.1895 - accuracy: 0.7412\n",
            "[INFO] Training model: epoch 1th 54000/1415032 samples\n",
            "10/10 - 58s - loss: 2.3343 - accuracy: 0.7174\n",
            "[INFO] Training model: epoch 1th 55000/1415032 samples\n",
            "10/10 - 58s - loss: 2.1679 - accuracy: 0.7428\n",
            "[INFO] Training model: epoch 1th 56000/1415032 samples\n",
            "10/10 - 58s - loss: 2.1503 - accuracy: 0.7445\n",
            "[INFO] Training model: epoch 1th 57000/1415032 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNu-n0aQQ5SD"
      },
      "source": [
        "# Training only if we chose training mode\n",
        "if MODE == 'train':\n",
        "    k_start = 1\n",
        "\n",
        "    # If any trained weight was found, then load them into the model\n",
        "    if len(saved_weights) != 0:\n",
        "        print('[INFO] Saved weights found, loading...')\n",
        "        epoch = saved_weights[saved_weights.rfind('_')+1:saved_weights.rfind('.')]\n",
        "        model.load_weights(saved_weights)\n",
        "        k_start = int(epoch) + 1\n",
        "\n",
        "    i_end = 0\n",
        "    for k in range(k_start, NB_EPOCH+1):\n",
        "        # Shuffling the training data every epoch to avoid local minima\n",
        "        indices = np.arange(len(X))\n",
        "        np.random.shuffle(indices)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        # Training 1000 sequences at a time\n",
        "        for i in range(0, len(X), 1000):\n",
        "            if i + 1000 >= len(X):\n",
        "                i_end = len(X)\n",
        "            else:\n",
        "                i_end = i + 1000\n",
        "            y_sequences = process_data(y[i:i_end], y_max_len, y_word_to_ix)\n",
        "\n",
        "            print('[INFO] Training model: epoch {}th {}/{} samples'.format(k, i, len(X)))\n",
        "            model.fit(X[i:i_end], y_sequences, batch_size=BATCH_SIZE, nb_epoch=1, verbose=2)\n",
        "        model.save_weights('checkpoint_epoch_{}.hdf5'.format(k))\n",
        "\n",
        "# Performing test if we chose test mode\n",
        "else:\n",
        "    # Only performing test if there is any saved weights\n",
        "    if len(saved_weights) == 0:\n",
        "        print(\"The network hasn't been trained! Program will exit...\")\n",
        "        sys.exit()\n",
        "    else:\n",
        "        X_test = load_test_data('test', X_word_to_ix, MAX_LEN)\n",
        "        X_test = pad_sequences(X_test, maxlen=X_max_len, dtype='int32')\n",
        "        model.load_weights(saved_weights)\n",
        "\n",
        "        predictions = np.argmax(model.predict(X_test), axis=2)\n",
        "        sequences = []\n",
        "        for prediction in predictions:\n",
        "            sequence = ' '.join([y_ix_to_word(index) for index in prediction if index > 0])\n",
        "            print(sequence)\n",
        "            sequences.append(sequence)\n",
        "        np.savetxt('test_result', sequences, fmt='%s')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}